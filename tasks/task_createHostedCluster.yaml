apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  name: create-hosted-cluster
spec:
  params:
    - name: targetManagedCluster
      default: local-cluster
      description: The managedcluster to target to become a managementcluster
      type: string
  results:
    - name: hdStatus
      description: Status of the hosted cluster
    - name: hostedClusterName
      description: Name of the hosted cluster
    - name: infraID
      description: infra id of the hosted cluster

  steps:
    - args:
        - |-
          #!/bin/bash

          ERROR_STATUS_MESSAGE="Failed to create hosted cluster"
          echo $ERROR_STATUS_MESSAGE > failureStatus.yaml
          {
          _LOGIN_CMD=$(cat login.sh)
          eval "$_LOGIN_CMD"

          OCP_RELEASE_IMAGE=quay.io/openshift-release-dev/ocp-release:4.12.6-x86_64
          OCP_PULL_SECRET=$(cat awsCreds.yaml | yq eval '.data.pullSecret' - )
          HOSTING_CLUSTER_NAME=$(inputs.params.targetManagedCluster)
          REGION=us-west-1
          BASE_DOMAIN=dev11.red-chesterfield.com
          EXT_DNS_DOMAIN=hs-pipeline.dev11.red-chesterfield.com
          S3_BUCKET_NAME=hypershift-ci-bucket
          CLUSTER_NAME_PREFIX=ge-
          # The AWS creds
          AWS_ACCESS_KEY_ID=$(cat awsCreds.yaml | yq eval '.data.aws_access_key_id' - | base64 -d)
          AWS_SECRET_ACCESS_KEY=$(cat awsCreds.yaml | yq eval '.data.aws_secret_access_key' - | base64 -d)

          echo "OCP_RELEASE_IMAGE $OCP_RELEASE_IMAGE"
          echo "OCP_PULL_SECRET $OCP_PULL_SECRET"
          echo "HOSTING_CLUSTER_NAME $HOSTING_CLUSTER_NAME"
          echo "REGION $REGION"
          echo "BASE_DOMAIN $BASE_DOMAIN"
          echo "EXT_DNS_DOMAIN $EXT_DNS_DOMAIN"
          echo "S3_BUCKET_NAME $S3_BUCKET_NAME"
          echo "CLUSTER_NAME_PREFIX $CLUSTER_NAME_PREFIX"
          echo "AWS_ACCESS_KEY_ID $AWS_ACCESS_KEY_ID"
          echo "AWS_SECRET_ACCESS_KEY $AWS_SECRET_ACCESS_KEY"

          # Create AWS credentials file
          mkdir ~/.aws
          cat <<EOF >~/.aws/credentials
          [default]
          aws_access_key_id=${AWS_ACCESS_KEY_ID}
          aws_secret_access_key=${AWS_SECRET_ACCESS_KEY}
          EOF

          AWS_CREDS_FILE=~/.aws/credentials

          echo "Aws Credentials file"

          # CLI variables
          # This value can be like "kubectl --kubeconfig my/hub/kubeconfig"
          KUBECTL_COMMAND="oc"
          # This value can be a different file path pinting to the hypershift CLI binary like "/my/dir/hypershift"
          HYPERSHIFT_COMMAND="hypershift"

          oc create secret generic hypershift-operator-oidc-provider-s3-credentials --from-file=credentials=${AWS_CREDS_FILE} --from-literal=bucket=${S3_BUCKET_NAME} --from-literal=region=us-east-1 -n local-cluster || true

          # Generate the hosted cluster name
          CLUSTER_NAME_1=${CLUSTER_NAME_PREFIX}$(cat /dev/urandom | env LC_ALL=C tr -dc 'a-z0-9' | fold -w 6 | head -n 1)
          INFRA_ID_1=$(cat /dev/urandom | env LC_ALL=C tr -dc 'a-z0-9' | fold -w 32 | head -n 1)
          CLUSTER_UUID_1=$(uuidgen)
          INFRA_OUTPUT_FILE_1=${CLUSTER_NAME_1}-infraout
          IAM_OUTPUT_FILE_1=${CLUSTER_NAME_1}-iam

          echo "$CLUSTER_NAME_1" | tr -d '\n' > $(results.hostedClusterName.path)
          echo "$INFRA_ID_1" | tr -d '\n' > $(results.infraID.path)


          createHostedCluster() {
              clusterName=$1
              infraID=$2
              uuid=$3
              infraOutfile=$4
              iamOutfile=$5

              declare -A vars

              vars[OCP_RELEASE_IMAGE]=${OCP_RELEASE_IMAGE}
              vars[OCP_PULL_SECRET]=${OCP_PULL_SECRET}
              vars[HOSTING_CLUSTER_NAME]=${HOSTING_CLUSTER_NAME}
              vars[REGION]=${REGION}
              vars[BASE_DOMAIN]=${BASE_DOMAIN}
              vars[EXT_DNS_DOMAIN]=${EXT_DNS_DOMAIN}
              vars[CLUSTER_NAME_PREFIX]=g${CLUSTER_NAME_PREFIX}
              vars[CLUSTER_NAME]=${clusterName}
              vars[INFRA_ID]=${infraID}
              vars[CLUSTER_UUID]=${uuid}
              vars[PRIVATE_KEY]=${PRIVATE_KEY}
              vars[PUBLIC_KEY]=${PUBLIC_KEY}

              echo "$(date) ==== Creating AWS infrastructure ===="
              echo "$(date) hypershift create infra aws --aws-creds ${AWS_CREDS_FILE} --base-domain ${vars[BASE_DOMAIN]} --infra-id ${vars[INFRA_ID]} --name ${vars[CLUSTER_NAME]} --region ${vars[REGION]} --output-file ${infraOutfile}"
              echo " --- AWS CREDS --- "
              cat $AWS_CREDS_FILE
              echo " --- AWS CREDS --- "

              # Create AWS infrastructure
              ${HYPERSHIFT_COMMAND} create infra aws --aws-creds ${AWS_CREDS_FILE} --base-domain ${vars[BASE_DOMAIN]} --infra-id ${vars[INFRA_ID]} --name ${vars[CLUSTER_NAME]} --region ${vars[REGION]} --output-file ${infraOutfile}
              if [ $? -ne 0 ]; then
                  echo "failed to create infra"
                  
                  exit 1
              fi

              # Set infra resource variables
              vars[MACHINE_CIDR]=$(cat ${infraOutfile} | jq '.machineCIDR' | tr -d '"')
              vars[VPC_ID]=$(cat ${infraOutfile} | jq '.vpcID' | tr -d '"')
              vars[ZONE_NAME]=$(cat ${infraOutfile} | jq '.zones[0] .name' | tr -d '"')
              vars[ZONE_SUBNET_ID]=$(cat ${infraOutfile} | jq '.zones[0] .subnetID' | tr -d '"')
              vars[SECURITY_GROUP_ID]=$(cat ${infraOutfile} | jq '.securityGroupID' | tr -d '"')
              vars[PUBLIC_ZONE_ID]=$(cat ${infraOutfile} | jq '.publicZoneID' | tr -d '"')
              vars[PRIVATE_ZONE_ID]=$(cat ${infraOutfile} | jq '.privateZoneID' | tr -d '"')
              vars[LOCAL_ZONE_ID]=$(cat ${infraOutfile} | jq '.localZoneID' | tr -d '"')

              echo "$(date) ==== Creating AWS IAM ===="
              echo "$(date) ${HYPERSHIFT_COMMAND} create iam aws --aws-creds ${AWS_CREDS_FILE} --infra-id ${vars[INFRA_ID]} --local-zone-id ${vars[LOCAL_ZONE_ID]} --private-zone-id ${vars[PRIVATE_ZONE_ID]} --public-zone-id ${vars[PUBLIC_ZONE_ID]} --oidc-storage-provider-s3-bucket-name ${S3_BUCKET_NAME} --oidc-storage-provider-s3-region us-east-1 --output-file ${iamOutfile}"

              # Create AWS IAM
              ${HYPERSHIFT_COMMAND} create iam aws --aws-creds ${AWS_CREDS_FILE} --infra-id ${vars[INFRA_ID]} --local-zone-id ${vars[LOCAL_ZONE_ID]} --private-zone-id ${vars[PRIVATE_ZONE_ID]} --public-zone-id ${vars[PUBLIC_ZONE_ID]} --oidc-storage-provider-s3-bucket-name ${S3_BUCKET_NAME} --oidc-storage-provider-s3-region us-east-1 --output-file ${iamOutfile}
              if [ $? -ne 0 ]; then
                  echo "$(date) Failed to create IAM"
                  echo "$(date) Destroying the AWS infrastructure"
                  ${HYPERSHIFT_COMMAND} destroy iam aws --region ${REGION} --aws-creds ${AWS_CREDS_FILE} --infra-id ${vars[INFRA_ID]}
                  
                  exit 1
              fi

              # Set iam resource variables
              vars[PROFILE_NAME]=$(cat ${iamOutfile} | jq '.profileName' | tr -d '"')
              vars[ISSUER_URL]=$(cat ${iamOutfile} | jq '.issuerURL' | tr -d '"')
              vars[ROLES_INGRESS_ARN]=$(cat ${iamOutfile} | jq '.roles .ingressARN' | tr -d '"')
              vars[ROLES_IMG_REGISTRY_ARN]=$(cat ${iamOutfile} | jq '.roles .imageRegistryARN' | tr -d '"')
              vars[ROLES_STORAGE_ARN]=$(cat ${iamOutfile} | jq '.roles .storageARN' | tr -d '"')
              vars[ROLES_NETWORK_ARN]=$(cat ${iamOutfile} | jq '.roles .networkARN' | tr -d '"')
              vars[ROLES_KUBE_CLOUD_CONTROLLER_ARN]=$(cat ${iamOutfile} | jq '.roles .kubeCloudControllerARN' | tr -d '"')
              vars[ROLES_NODEPOOL_MGMT_ARN]=$(cat ${iamOutfile} | jq '.roles .nodePoolManagementARN' | tr -d '"')
              vars[ROLES_CPO_ARN]=$(cat ${iamOutfile} | jq '.roles .controlPlaneOperatorARN' | tr -d '"')

              # Copy the template hostedcluster nodepool manifestwork YAML
              cp ./resources/hosted_cluster_manifestwork.yaml ./${vars[CLUSTER_NAME]}.yaml
              echo "Copying " ./resources/hosted_cluster_manifestwork.yaml "to" ./${vars[CLUSTER_NAME]}.yaml
              echo ./${vars[CLUSTER_NAME]}.yaml
              if [ $? -ne 0 ]; then
                  echo "$(date) failed to copy hosted_cluster_manifestwork.yaml"
                  echo "$(date) Destroying the AWS infrastructure and IAM"
                  ${HYPERSHIFT_COMMAND} destroy iam aws --region ${REGION} --aws-creds ${AWS_CREDS_FILE} --infra-id ${vars[INFRA_ID]}
                  ${HYPERSHIFT_COMMAND} destroy infra aws --region ${REGION} --aws-creds ${AWS_CREDS_FILE} --base-domain $vars[BASE_DOMAIN] --infra-id ${vars[INFRA_ID]}
                  
                  exit 1
              fi

              # Copy the template htpasswd manifestwork YAML
              cp ./resources/htpasswd.yaml ./${vars[CLUSTER_NAME]}-htpasswd.yaml
              echo "Copying" ./resources/htpasswd.yaml "to" ./${vars[CLUSTER_NAME]}-htpasswd.yaml
              if [ $? -ne 0 ]; then
                  echo "$(date) failed to copy htpasswd.yaml"
                  echo "$(date) Destroying the AWS infrastructure and IAM"
                  ${HYPERSHIFT_COMMAND} destroy iam aws --region ${REGION} --aws-creds ${AWS_CREDS_FILE} --infra-id ${vars[INFRA_ID]}
                  ${HYPERSHIFT_COMMAND} destroy infra aws --region ${REGION} --aws-creds ${AWS_CREDS_FILE} --base-domain $vars[BASE_DOMAIN] --infra-id ${vars[INFRA_ID]}
                  
                  exit 1
              fi

              # Copy the template managedcluster YAML
              cp ./resources/managedcluster.yaml ./${vars[CLUSTER_NAME]}-managedcluster.yaml
              echo "Copying" ./resources/managedcluster.yaml "to" ./${vars[CLUSTER_NAME]}-managedcluster.yaml
              if [ $? -ne 0 ]; then
                  echo "$(date) failed to copy managedcluster.yaml"
                  echo "$(date) Destroying the AWS infrastructure and IAM"
                  ${HYPERSHIFT_COMMAND} destroy iam aws --region ${REGION} --aws-creds ${AWS_CREDS_FILE} --infra-id ${vars[INFRA_ID]}
                  ${HYPERSHIFT_COMMAND} destroy infra aws --region ${REGION} --aws-creds ${AWS_CREDS_FILE} --base-domain $vars[BASE_DOMAIN] --infra-id ${vars[INFRA_ID]}
                  
                  exit 1
              fi

              # Replace variables with the actual infra and iam values in the manifestworks and managedcluster
              for key in ${!vars[@]}
                  do
                      value=${vars[${key}]}
                      sed -i -e "s|__${key}__|${value}|" ${vars[CLUSTER_NAME]}.yaml
                      if [ $? -ne 0 ]; then
                          echo "$(date) failed to substitue __${key}__ in ${vars[CLUSTER_NAME]}.yaml"
                          echo "$(date) Destroying the AWS infrastructure and IAM"
                          ${HYPERSHIFT_COMMAND} destroy iam aws --region ${REGION} --aws-creds ${AWS_CREDS_FILE} --infra-id ${vars[INFRA_ID]}
                          ${HYPERSHIFT_COMMAND} destroy infra aws --region ${REGION} --aws-creds ${AWS_CREDS_FILE} --base-domain $vars[BASE_DOMAIN] --infra-id ${vars[INFRA_ID]}
                          
                          exit 1
                      fi

                      sed -i -e "s|__${key}__|${value}|" ${vars[CLUSTER_NAME]}-htpasswd.yaml
                      if [ $? -ne 0 ]; then
                          echo "$(date) failed to substitue __${key}__ in ${vars[CLUSTER_NAME]}-htpasswd.yaml"
                          echo "$(date) Destroying the AWS infrastructure and IAM"
                          ${HYPERSHIFT_COMMAND} destroy iam aws --region ${REGION} --aws-creds ${AWS_CREDS_FILE} --infra-id ${vars[INFRA_ID]}
                          ${HYPERSHIFT_COMMAND} destroy infra aws --region ${REGION} --aws-creds ${AWS_CREDS_FILE} --base-domain $vars[BASE_DOMAIN] --infra-id ${vars[INFRA_ID]}
                          
                          exit 1
                      fi

                      sed -i -e "s|__${key}__|${value}|" ${vars[CLUSTER_NAME]}-managedcluster.yaml
                      if [ $? -ne 0 ]; then
                          echo "$(date) failed to substitue __${key}__ in ${vars[CLUSTER_NAME]}-managedcluster.yaml"
                          echo "$(date) Destroying the AWS infrastructure and IAM"
                          ${HYPERSHIFT_COMMAND} destroy iam aws --region ${REGION} --aws-creds ${AWS_CREDS_FILE} --infra-id ${vars[INFRA_ID]}
                          ${HYPERSHIFT_COMMAND} destroy infra aws --region ${REGION} --aws-creds ${AWS_CREDS_FILE} --base-domain $vars[BASE_DOMAIN] --infra-id ${vars[INFRA_ID]}
                          
                          exit 1
                      fi
                  done
                  
              # Apply the managedcluster and manifestworks to get the hosted cluster created in the remote hosting cluster
              ${KUBECTL_COMMAND} apply -f ${vars[CLUSTER_NAME]}-managedcluster.yaml
              if [ $? -ne 0 ]; then
                  echo "$(date) failed to apply ${vars[CLUSTER_NAME]}-managedcluster.yaml"
                  echo "$(date) Destroying the AWS infrastructure and IAM"
                  ${HYPERSHIFT_COMMAND} destroy iam aws --region ${REGION} --aws-creds ${AWS_CREDS_FILE} --infra-id ${vars[INFRA_ID]}
                  ${HYPERSHIFT_COMMAND} destroy infra aws --region ${REGION} --aws-creds ${AWS_CREDS_FILE} --base-domain $vars[BASE_DOMAIN] --infra-id ${vars[INFRA_ID]}
                  
                  exit 1
              fi

              ${KUBECTL_COMMAND} apply -f ${vars[CLUSTER_NAME]}.yaml
              if [ $? -ne 0 ]; then
                  echo "$(date) failed to apply ${vars[CLUSTER_NAME]}.yaml"
                  echo "$(date) Destroying the AWS infrastructure and IAM"
                  ${HYPERSHIFT_COMMAND} destroy iam aws --region ${REGION} --aws-creds ${AWS_CREDS_FILE} --infra-id ${vars[INFRA_ID]}
                  ${HYPERSHIFT_COMMAND} destroy infra aws --region ${REGION} --aws-creds ${AWS_CREDS_FILE} --base-domain $vars[BASE_DOMAIN] --infra-id ${vars[INFRA_ID]}
                  
                  exit 1
              fi

              ${KUBECTL_COMMAND} apply -f ${vars[CLUSTER_NAME]}-htpasswd.yaml
              if [ $? -ne 0 ]; then
                  echo "$(date) failed to apply ${vars[CLUSTER_NAME]}-htpasswd.yaml"
                  echo "$(date) Destroying the AWS infrastructure and IAM"
                  ${HYPERSHIFT_COMMAND} destroy iam aws --region ${REGION} --aws-creds ${AWS_CREDS_FILE} --infra-id ${vars[INFRA_ID]}
                  ${HYPERSHIFT_COMMAND} destroy infra aws --region ${REGION} --aws-creds ${AWS_CREDS_FILE} --base-domain $vars[BASE_DOMAIN] --infra-id ${vars[INFRA_ID]}
                  
                  exit 1
              fi
          }

          verifyHostedCluster() {
              FOUND=1
              SECONDS=0
              infraId=$1

              managedClusterImported=false  
              hostedClusterCompleted=false
              nodePoolReady=false

              while [ ${FOUND} -eq 1 ]; do
                  # Wait up to 45 minutes, re-try every 30 seconds
                  if [ $SECONDS -gt 2700 ]; then
                      echo "$(date) Timeout waiting for a successful provisioning of hosted cluster."
                      ${KUBECTL_COMMAND} get managedcluster ${infraId} -o yaml
                      echo "$(date) Destroying the AWS infrastructure and IAM"
                      ${HYPERSHIFT_COMMAND} destroy iam aws --region ${REGION} --aws-creds ${AWS_CREDS_FILE} --infra-id ${infraId}
                      ${HYPERSHIFT_COMMAND} destroy infra aws --region ${REGION} --aws-creds ${AWS_CREDS_FILE} --base-domain ${BASE_DOMAIN} --infra-id ${infraId}
                      
                      exit 1
                  fi

                  # Wait for the managed cluster to become joined and available
                  HubAcceptedManagedCluster=`${KUBECTL_COMMAND} get managedcluster ${infraId} -o jsonpath='{.status.conditions[?(@.type=="HubAcceptedManagedCluster")].status}'`
                  ManagedClusterJoined=`${KUBECTL_COMMAND} get managedcluster ${infraId} -o jsonpath='{.status.conditions[?(@.type=="ManagedClusterJoined")].status}'`
                  ManagedClusterConditionAvailable=`${KUBECTL_COMMAND} get managedcluster ${infraId} -o jsonpath='{.status.conditions[?(@.type=="ManagedClusterConditionAvailable")].status}'`
                  ManagedClusterURL=`${KUBECTL_COMMAND} get managedcluster ${infraId} -o jsonpath='{.spec.managedClusterClientConfigs[0].url}'`

                  if [[ ("$HubAcceptedManagedCluster" == "True") && ("$ManagedClusterJoined" == "True") && ("$ManagedClusterConditionAvailable" == "True") && ("$ManagedClusterURL" > "") ]]; then
                      echo "$(date) Managed cluster: imported"
                      managedClusterImported=true
                  else
                      echo "$(date) Managed cluster: pending import"
                  fi

                  # Check the manifestwork status feedback to verify that the hosted cluster is avaiable
                  HostedClusterStatusFeedback=`${KUBECTL_COMMAND} get manifestwork ${infraId} -n ${HOSTING_CLUSTER_NAME} -o jsonpath='{.status.resourceStatus}' | jq '.manifests[] | select(.resourceMeta.kind=="HostedCluster").statusFeedback.values[]'`
                  overallProgressStatus=`echo ${HostedClusterStatusFeedback} | jq 'select(.name=="progress").fieldValue.string'`
                  hcpAvailableStatus=`echo ${HostedClusterStatusFeedback} | jq 'select(.name=="Available-Status").fieldValue.string'`
                  progressingStatus=`echo ${HostedClusterStatusFeedback} | jq 'select(.name=="Progressing-Status").fieldValue.string'`
                  degradedStatus=`echo ${HostedClusterStatusFeedback} | jq 'select(.name=="Degraded-Status").fieldValue.string'`
                  ignitionEndpointStatus=`echo ${HostedClusterStatusFeedback} | jq 'select(.name=="IgnitionEndpointAvailable-Status").fieldValue.string'`
                  infraReadyStatus=`echo ${HostedClusterStatusFeedback} | jq 'select(.name=="InfrastructureReady-Status").fieldValue.string'`
                  kubeAPIServerReadyStatus=`echo ${HostedClusterStatusFeedback} | jq 'select(.name=="KubeAPIServerAvailable-Status").fieldValue.string'`

                  if [[ ("$overallProgressStatus" == "\"Completed\"") && \
                          ("$hcpAvailableStatus" == "\"True\"") && \
                          ("$progressingStatus" == "\"False\"") && \
                          ("$degradedStatus" == "\"False\"") && \
                          ("$ignitionEndpointStatus" == "\"True\"") && \
                          ("$infraReadyStatus" == "\"True\"") && \
                          ("$kubeAPIServerReadyStatus" == "\"True\"") ]]; then
                      echo "$(date) HostedCluster: ${overallProgressStatus}"
                      hostedClusterCompleted=true
                  else
                      echo "$(date) HostedCluster: ${overallProgressStatus}"
                  fi

                  # Check the manifestwork status feedback to verify that the node pool is avaiable
                  NpdePoolStatusFeedback=`${KUBECTL_COMMAND} get manifestwork ${infraId} -n ${HOSTING_CLUSTER_NAME} -o jsonpath='{.status.resourceStatus}' | jq '.manifests[] | select(.resourceMeta.kind=="NodePool").statusFeedback.values[]'`
                  readyStatus=`echo ${NpdePoolStatusFeedback} | jq 'select(.name=="Ready-Status").fieldValue.string'`

                  if [[ ("$readyStatus" == "\"True\"") ]]; then
                      echo "$(date) NodePool: ready"
                      nodePoolReady=true
                  else
                      echo "$(date) NodePool: not ready"
                  fi

                  if [[ ("$managedClusterImported" == true) && ("$hostedClusterCompleted" == true) && ("$nodePoolReady" == true) ]]; then
                      break
                  fi

                  sleep 30
                  (( SECONDS = SECONDS + 30 ))
              done
          }

          waitForManagedClusterDelete() {
              FOUND=1
              SECONDS=0

              resName=$1

              while [ ${FOUND} -eq 1 ]; do
                  # Wait up to 30 minutes
                  if [ $SECONDS -gt 1800 ]; then
                      echo "$(date) Timed out waiting for managed cluster ${resName} to be deleted."
                      ${KUBECTL_COMMAND} get managedcluster ${resName} -o yaml
                      ${HYPERSHIFT_COMMAND} destroy iam aws --region ${REGION} --aws-creds ${AWS_CREDS_FILE} --infra-id ${vars[INFRA_ID]}
                      ${HYPERSHIFT_COMMAND} destroy infra aws --region ${REGION} --aws-creds ${AWS_CREDS_FILE} --base-domain $vars[BASE_DOMAIN] --infra-id ${vars[INFRA_ID]}
                      
                      exit 1
                  fi

                  ${KUBECTL_COMMAND} get managedcluster ${resName}
                  if [ $? -eq 0 ]; then
                      echo "$(date) managed cluster ${resName} still exists"
                  else
                      echo "$(date) managed cluster ${resName} not found"
                      break
                  fi

                  sleep 30
                  (( SECONDS = SECONDS + 30 ))
              done
          }


          enableHypershiftForLocalCluster() {
              ${KUBECTL_COMMAND} create secret generic hypershift-operator-external-dns-credentials --from-file=credentials=${AWS_CREDS_FILE} --from-literal=provider=aws --from-literal=domain-filter=${EXT_DNS_DOMAIN} -n local-cluster
              if [ $? -ne 0 ]; then
                  echo "$(date) failed to create secret hypershift-operator-external-dns-credentials"
              fi

              # Wait for hypershift-addon to be available
              FOUND=1
              SECONDS=0
              running="\([0-9]\+\)\/\1"
              while [ ${FOUND} -eq 1 ]; do
                  # Wait up to 10min
                  if [ $SECONDS -gt 600 ]; then
                      echo "Timeout waiting for hypershift-addon to be available."
                      echo "List of current pods:"
                      ${KUBECTL_COMMAND} get managedclusteraddon hypershift-addon -n local-cluster -o yaml
                      
                      exit 1
                  fi

                  addonAvailable=`${KUBECTL_COMMAND} get managedclusteraddon hypershift-addon -n local-cluster -o jsonpath='{.status.conditions[?(@.type=="Available")].status}'`
                  addonDegraded=`${KUBECTL_COMMAND} get managedclusteraddon hypershift-addon -n local-cluster -o jsonpath='{.status.conditions[?(@.type=="Degraded")].status}'`

                  if [[ ("$addonAvailable" == "True") && ("$addonDegraded" == "False" || "$addonDegraded" == "") ]]; then 
                      echo "Hypershift addon is available"
                      break
                  fi
                  sleep 10
                  (( SECONDS = SECONDS + 10 ))
              done
          }

          installHypershiftBinary() {
              SECONDS=0
              while : 
              do
                  if [ $SECONDS -gt 600 ]; then
                      echo "$(date) Timeout waiting for hypershift-cli-download to be available."
                      
                      exit 1
                  fi

                  ${KUBECTL_COMMAND} get ConsoleCLIDownload hypershift-cli-download
                  if [ $? -eq 0 ]; then
                      echo "$(date) ConsoleCLIDownload hypershift-cli-download is available"
                      sleep 60
                      break
                  fi

                  sleep 10
                  (( SECONDS = SECONDS + 10 ))
              done

              hypershiftTarGzURL=`${KUBECTL_COMMAND} get ConsoleCLIDownload hypershift-cli-download -o jsonpath='{.spec.links[?(@.text=="Download hypershift CLI for Linux for x86_64")].href}'`
              echo "hypershiftTarGzURL $hypershiftTarGzURL"
              if [ -z "$hypershiftTarGzURL" ]; then
                      echo "$(date) failed to get Hypershift tar.gz ConsoleCLIDownload hypershift-cli-download"
                  
                  exit 1
              fi

              curl -LOk ${hypershiftTarGzURL}
              if [ $? -ne 0 ]; then
                  echo "$(date) failed to download ${hypershiftTarGzURL}"
                  
                  exit 1
              fi

              tar xvzf hypershift.tar.gz
              if [ $? -ne 0 ]; then
                  echo "$(date) failed to untar hypershift.tar.gz"
                  
                  exit 1
              fi

              mv hypershift /bin
              if [ $? -ne 0 ]; then
                  echo "$(date) failed to mv extracted hypershift binary to /bin"
                  
                  exit 1
              fi

              chmod +x /bin/hypershift
              if [ $? -ne 0 ]; then
                  echo "$(date) failed to chmod +x /bin/hypershift"
                  
                  exit 1
              fi
          }

          enableHostedModeAddon() {
              ${KUBECTL_COMMAND} apply -f resources/addonconfig.yaml
              if [ $? -ne 0 ]; then
                  echo "$(date) failed to apply resources/addonconfig.yaml"
                  
                  exit 1
              fi

              ${KUBECTL_COMMAND} patch clustermanagementaddon work-manager --type merge -p '{"spec":{"supportedConfigs":[{"defaultConfig":{"name":"addon-hosted-config","namespace":"multicluster-engine"},"group":"addon.open-cluster-management.io","resource":"addondeploymentconfigs"}]}}'
              if [ $? -ne 0 ]; then
                  echo "$(date) failed to patch clustermanagementaddon work-manager"
                  
                  exit 1
              fi

              ${KUBECTL_COMMAND} patch clustermanagementaddon config-policy-controller --type merge -p '{"spec":{"supportedConfigs":[{"defaultConfig":{"name":"addon-hosted-config","namespace":"multicluster-engine"},"group":"addon.open-cluster-management.io","resource":"addondeploymentconfigs"}]}}'

              ${KUBECTL_COMMAND} patch clustermanagementaddon cert-policy-controller --type merge -p '{"spec":{"supportedConfigs":[{"defaultConfig":{"name":"addon-hosted-config","namespace":"multicluster-engine"},"group":"addon.open-cluster-management.io","resource":"addondeploymentconfigs"}]}}'
          }

          cloneHypershiftAddonRepo() {
              rm -rf hypershift-addon-operator
              git clone https://github.com/stolostron/hypershift-addon-operator
              if [ $? -ne 0 ]; then
                  echo "$(date) failed to clone hypershift-addon-operator repo"
              fi
              
              cd hypershift-addon-operator/test/canary
          }

          echo "$(date) ==== Enable hypershift feature ===="
          enableHypershiftForLocalCluster

          echo "$(date) ==== Installing hypershift binary ===="
          oc cluster-info
          oc whoami
          oc get deployment -n multicluster-engine

          installHypershiftBinary

          cloneHypershiftAddonRepo

          # Enabled hosted mode addons
          # https://github.com/stolostron/hypershift-addon-operator/blob/main/docs/advanced/running_mce_acm_addons_hostedmode.md
          echo "$(date) ==== Enable hosted mode addon configuration ===="
          enableHostedModeAddon

          # Generate AWS infrastructure and IAM for the hosted cluster
          # Generate the follwing YAMLs:
          #   - manifestwork YAML containing HostedCluster and NodePool
          #   - manifestwork YAML containing htpasswd for the hosted cluster (OCP user identify provider)
          #   - managed cluster YAML to import the hosted cluster
          # Then apply them to create a hosted cluster
          echo "$(date) ==== Creating hosted cluster  ${CLUSTER_NAME_1} ===="
          createHostedCluster ${CLUSTER_NAME_1} ${INFRA_ID_1} ${CLUSTER_UUID_1} ${INFRA_OUTPUT_FILE_1} ${IAM_OUTPUT_FILE_1}

          sleep 30

          # Verify that the managed cluster is imported, hosted cluster and node pool are available
          # This also verifies that we can log into the hosted cluster's API server using the user defined in htpasswd
          echo "$(date) ==== Verifying hosted cluster  ${CLUSTER_NAME_1} ===="
          verifyHostedCluster ${INFRA_ID_1}

          echo "-----------------HOSTED CLUSTER CREATION SUCCESSFUL-----------------"

          exit 0
          } 2>&1 | tee task_logs/$(context.task.name).log
      command:
        - /bin/bash
        - -c
      image: quay.io/zkayyali812/ocm-utils:latest
      name: apply
      resources: {}
      workingDir: /workspace/source
  workspaces:
    - name: source
